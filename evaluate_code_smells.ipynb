{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa765ae8",
   "metadata": {},
   "source": [
    "## Load Required Libraries\n",
    "We'll use pandas, matplotlib, openai, etc.\n",
    "Created using jupytext evaluate_code_smells.py --to notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061ecc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import openai\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from code_smell_utils import ANGULAR_SMELLS, DOTNET_SMELLS, extract_smells_from_response\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "import httpx\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eaa4e06",
   "metadata": {},
   "source": [
    "## Environment variables \n",
    "For security these are not added to the source code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4b6f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = \"code_review_samples_smole.csv\"\n",
    "\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAPITOKEN\")  \n",
    "OPEN_API_BASE_URL = os.getenv(\"OPENAPIBASEURL\")\n",
    "\n",
    "\n",
    "if not OPENAI_API_KEY:\n",
    "    raise EnvironmentError(\"OPENAIAPITOKEN environment variable not set\")\n",
    "\n",
    "if not OPEN_API_BASE_URL:\n",
    "    raise EnvironmentError(\"OPENAPIBASEURL environment variable not set\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0361717",
   "metadata": {},
   "source": [
    "## Define OpenAI API \n",
    "Server has a self-signed cert, for testing only, need to point the openAPI client to it and avoid errors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71bb20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "http_client = httpx.Client(verify=False)\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "openai.base_url = OPEN_API_BASE_URL\n",
    "openai.http_client = http_client\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9504e07f",
   "metadata": {},
   "source": [
    "## Define available and relevant models for testing\n",
    "\n",
    "Define experimental variables\n",
    "Best Run was llama-3-1-8b-instruct .89\n",
    "Second Best Run was llama-3-8b-instruct .78\n",
    "Third Place was was llama-3-sqlcoder-8b .73\n",
    "\n",
    "MODELS = [\n",
    "    # \"mixtral-8x7b-instruct-v01\",\n",
    "    # \"mistral-7b-instruct-v03\",\n",
    "    # \"llama-3-8b-instruct\",\n",
    "     \"llama-3-1-8b-instruct\",\n",
    "    # \"llama-3-2-3b-instruct\",\n",
    "    # \"phi-3-mini-128k-instruct\",\n",
    "    # \"phi-3-5-moe-instruct\",\n",
    "    # \"llama-3-3-70b-instruct\",\n",
    "    # \"codellama-13b-instruct\",\n",
    "    # \"llama-3-sqlcoder-8b\"\n",
    "]\n",
    "\n",
    "CODE_INPUT_TYPES = [\"diff_only\", \"full_file_plus_diff\"]\n",
    "PROMPT_TYPES = [\"short\", \"full\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50c579b",
   "metadata": {},
   "source": [
    "## Define Prompts\n",
    "Short general prompt and more well defined long prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c721cd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(row, code_input_type, prompt_type):\n",
    "    short_prompt = \"\"\"\n",
    "You are a code reviewer. Identify any code smells in the provided code.\n",
    "\n",
    "Return the list of issues as a markdown table with two columns:\n",
    "| Smell | Description |\n",
    "\n",
    "Use the following smell types when applicable: async, exception, duplicate_code, null_check, deep_nesting, structural_duplication, unused_code, hardcoded_localhost, commented_code.\n",
    "\n",
    "If no smells are found, return an empty markdown table.\n",
    "\"\"\"\n",
    "    full_prompt = \"\"\"\n",
    "You are an enterprise code assistant. Carefully review the following code for any issues based on best practices:\n",
    "\n",
    "- Avoid code duplication (DRY)\n",
    "- Use null-conditional operators\n",
    "- Minimize deep nesting\n",
    "- Prefer async/await patterns\n",
    "- Proper exception handling\n",
    "- Remove commented-out or testing code\n",
    "- Avoid hardcoded localhost or credentials\n",
    "- In Angular, unsubscribe from observables to prevent memory leaks\n",
    "- Follow proper state management practices using ngrx (avoid updating services directly)\n",
    "\n",
    "Return the results as a markdown table with two columns:\n",
    "| Smell | Description |\n",
    "\n",
    "Use one of the following predefined smell types in the Smell column:\n",
    "duplicate_code, null_check, deep_nesting, async, exception, structural_duplication, unused_code, hardcoded_localhost, commented_code, unsubscribed_observable, state_management_violation.\n",
    "\n",
    "Use each smell type only if it clearly applies. If there are no issues, return an empty table.\n",
    "\"\"\"\n",
    "    system_prompt = short_prompt if prompt_type == 'short' else full_prompt\n",
    "    \n",
    "    full_file = str(row.get(\"full_file\", \"\") or \"\")\n",
    "    file_diff = str(row.get(\"file_diff\", \"\") or \"\")\n",
    "\n",
    "    if code_input_type == 'full_file_plus_diff':\n",
    "        user_content = full_file + \"\\n\\n\" + file_diff\n",
    "    else:\n",
    "        user_content = file_diff\n",
    "    \n",
    "    return system_prompt, user_content\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960cebb3",
   "metadata": {},
   "source": [
    "## Experiment Definition\n",
    "Call the LLM to determine if there is a code smell or not under various conditions: short prompt vs long prompt\n",
    "Code without any issues, with issues, both Angular and C# DotNet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a099017",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "    results = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        expected_smells = [] if row['expected_smells'] == 'none' else row['expected_smells'].split(',')\n",
    "\n",
    "        for model, code_input_type, prompt_type in itertools.product(MODELS, CODE_INPUT_TYPES, PROMPT_TYPES):\n",
    "            system_prompt, user_content = build_prompt(row, code_input_type, prompt_type)\n",
    "            try:\n",
    "                print(f\"row {row['id']} using {model}, {prompt_type}\")\n",
    "                response = openai.completions.create(\n",
    "                    model=model,\n",
    "                    prompt= system_prompt + \"\\n\" + user_content,\n",
    "                    max_tokens=2000,\n",
    "                    temperature=0.1\n",
    "                )\n",
    "                reply = response.choices[0].text.strip()\n",
    "                detected_smells = extract_smells_from_response(reply, row['language'])\n",
    "\n",
    "                ALL_SMELLS = list(set(DOTNET_SMELLS + ANGULAR_SMELLS))\n",
    "\n",
    "                true_positives = set(expected_smells) & set(detected_smells)\n",
    "                false_negatives = set(expected_smells) - set(detected_smells)\n",
    "                false_positives = set(detected_smells) - set(expected_smells)\n",
    "                true_negatives = set(ALL_SMELLS) - (true_positives | false_positives | false_negatives)\n",
    "\n",
    "                results.append({\n",
    "                    \"id\": row['id'],\n",
    "                    \"language\": row['language'],\n",
    "                    \"model\": model,\n",
    "                    \"code_input_type\": code_input_type,\n",
    "                    \"prompt_type\": prompt_type,\n",
    "                    \"true_positives\": len(true_positives),\n",
    "                    \"true_negatives\": len(true_negatives),\n",
    "                    \"false_positives\": len(false_positives),\n",
    "                    \"false_negatives\": len(false_negatives),\n",
    "                    \"expected\": expected_smells,\n",
    "                    \"detected\": detected_smells\n",
    "                })\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error with row {row['id']} using {model}, {code_input_type}, {prompt_type}: {e}\")\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6f1fb5",
   "metadata": {},
   "source": [
    "## ROC Plot Definition\n",
    "Plot ROC - Receiver Operating Characteristic \n",
    "Values closer to 1 means more true positives less false positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2ee3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_by_model_and_prompt(df):\n",
    "    models = df['model'].unique()\n",
    "    prompt_types = df['prompt_type'].unique()\n",
    "    all_smells = list(set().union(*df['expected']).union(*df['detected']))\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    for model in models:\n",
    "        for prompt_type in prompt_types:\n",
    "            subset = df[(df['model'] == model) & (df['prompt_type'] == prompt_type)]\n",
    "            y_true, y_score = [], []\n",
    "\n",
    "            for _, row in subset.iterrows():\n",
    "                expected = set(row[\"expected\"])\n",
    "                detected = set(row[\"detected\"])\n",
    "                for smell in all_smells:\n",
    "                    y_true.append(1 if smell in expected else 0)\n",
    "                    y_score.append(1 if smell in detected else 0)\n",
    "\n",
    "            if any(y_true):  # only plot if there are positives\n",
    "                fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "                roc_auc = auc(fpr, tpr)\n",
    "                label = f\"{model} ({prompt_type}) AUC={roc_auc:.2f}\"\n",
    "                plt.plot(fpr, tpr, label=label)\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Random (AUC = 0.5)')\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curve by Model and Prompt Type\")\n",
    "    plt.legend(loc=\"lower right\", fontsize=\"small\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6f4f9e",
   "metadata": {},
   "source": [
    "## Run the program\n",
    "Run and plot the result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfe393b",
   "metadata": {
    "lines_to_next_cell": 3
   },
   "outputs": [],
   "source": [
    "results_df = evaluate()\n",
    "plot_roc_by_model_and_prompt(results_df)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
